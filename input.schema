{
  "functions": [
    {
      "name": "Classify",
      "extensions": [
        {
          "extension": "jpg"
        }
      ],
      "doctxt": "This simply classifies the image using the specified model, no attack here.",
      "options": [
        {
          "name": "model",
          "type": "enum",
          "values": [
            "vgg16",
            "resnet18"
          ],
          "required": true,
          "doctxt": "Pre-trained model to use for classification."
        }
      ]
    },
    {
      "name": "BasicIterativeMethod",
      "extensions": [
        {
          "extension": "jpg"
        }
      ],
      "doctxt": "The Basic Iterative Method introduced in [1].\n\nThis attack is also known as Projected Gradient Descent (PGD) (without random start) or FGMS^k.\n\nReferences\n[1] Alexey Kurakin, Ian Goodfellow, Samy Bengio, \"Adversarial examples in the physical world\", https://arxiv.org/abs/1607.02533",
      "options": [
        {
          "name": "model",
          "type": "enum",
          "values": [
            "vgg16",
            "resnet18"
          ],
          "required": true,
          "doctxt": "Pre-trained model to be attacked."
        },
        {
          "name": "binary_search",
          "type": "bool",
          "default": true,
          "required": true,
          "doctxt": "Whether to perform a binary search over epsilon and stepsize, keeping their ratio constant and using their values to start the search. If False, hyperparameters are not optimized."
        },
        {
          "name": "epsilon",
          "type": "float",
          "default": 0.3,
          "required": true,
          "doctxt": "Limit on the perturbation size; if binary_search is True, this value is only for initialization and automatically adapted."
        },
        {
          "name": "stepsize",
          "type": "float",
          "default": 0.05,
          "required": true,
          "doctxt": "Step size for gradient descent; if binary_search is True, this value is only for initialization and automatically adapted."
        },
        {
          "name": "iterations",
          "type": "int",
          "default": 10,
          "required": true,
          "doctxt": "Number of iterations for each gradient descent run."
        },
        {
          "name": "random_start",
          "type": "bool",
          "default": false,
          "required": true,
          "doctxt": "Start the attack from a random point rather than from the original input."
        },
        {
          "name": "return_early",
          "type": "bool",
          "default": true,
          "required": true,
          "doctxt": "Whether an individual gradient descent run should stop as soon as an adversarial is found."
        }
      ]
    },
    {
      "name": "CarliniWagnerL2Attack",
      "extensions": [
        {
          "extension": "jpg"
        }
      ],
      "doctxt": "The L2 version of the Carlini & Wagner attack.\n\nThis attack is described in [1]. This implementation is based on the reference implementation by Carlini [2]. For bounds != (0, 1), it differs from [2] because we normalize the squared L2 loss with the bounds.\n\nReferences\n[1] Nicholas Carlini, David Wagner: \"Towards Evaluating the Robustness of Neural Networks\", https://arxiv.org/abs/1608.04644\n[2] (1,2) https://github.com/carlini/nn_robust_attacks",
      "options": [
        {
          "name": "model",
          "type": "enum",
          "values": [
            "vgg16",
            "resnet18"
          ],
          "required": true,
          "doctxt": "Pre-trained model to be attacked."
        },
        {
          "name": "binary_search_steps",
          "type": "int",
          "default": 5,
          "required": true,
          "doctxt": "The number of steps for the binary search used to find the optimal tradeoff-constant between distance and confidence."
        },
        {
          "name": "max_iterations",
          "type": "int",
          "default": 1000,
          "required": true,
          "doctxt": "The maximum number of iterations. Larger values are more accurate; setting it too small will require a large learning rate and will produce poor results."
        },
        {
          "name": "confidence",
          "type": "int",
          "default": 0,
          "required": true,
          "doctxt": "Confidence of adversarial examples: a higher value produces adversarials that are further away, but more strongly classified as adversarial."
        },
        {
          "name": "learning_rate",
          "type": "float",
          "default": 0.005,
          "required": true,
          "doctxt": "The learning rate for the attack algorithm. Smaller values produce better results but take longer to converge."
        },
        {
          "name": "initial_const",
          "type": "float",
          "default": 0.01,
          "required": true,
          "doctxt": "The initial tradeoff-constant to use to tune the relative importance of distance and confidence. If binary_search_steps is large, the initial constant is not important."
        },
        {
          "name": "abort_early",
          "type": "bool",
          "default": true,
          "required": true,
          "doctxt": "If True, Adam will be aborted if the loss hasn't decreased for some time (a tenth of max_iterations)."
        }
      ]
    },
    {
      "name": "DeepFoolL2Attack",
      "extensions": [
        {
          "extension": "jpg"
        }
      ],
      "doctxt": "Simple and close to optimal gradient-based adversarial attack.",
      "options": [
        {
          "name": "model",
          "type": "enum",
          "values": [
            "vgg16",
            "resnet18"
          ],
          "required": true,
          "doctxt": "Pre-trained model to be attacked."
        },
        {
          "name": "steps",
          "type": "int",
          "default": 100,
          "required": true,
          "doctxt": "Maximum number of steps to perform."
        },
        {
          "name": "subsample",
          "type": "int",
          "default": 10,
          "required": true,
          "doctxt": "Limit on the number of the most likely classes that should be considered. A small value is usually sufficient and much faster."
        }
      ]
    },
    {
      "name": "FGSM",
      "extensions": [
        {
          "extension": "jpg"
        }
      ],
      "doctxt": "Adds the sign of the gradient to the input, gradually increasing the magnitude until the input is misclassified. This attack is often referred to as Fast Gradient Sign Method and was introduced in [1].\n\nDoes not do anything if the model does not have a gradient.\n\nReferences\n[1]\tIan J. Goodfellow, Jonathon Shlens, Christian Szegedy, \u201cExplaining and Harnessing Adversarial Examples\u201d, https://arxiv.org/abs/1412.6572",
      "options": [
        {
          "name": "model",
          "type": "enum",
          "values": [
            "vgg16",
            "resnet18"
          ],
          "required": true,
          "doctxt": "Pre-trained model to be attacked."
        },
        {
          "name": "epsilons",
          "type": "int",
          "default": 1000,
          "required": true,
          "doctxt": "Number of step sizes between 0 and max_epsilon that should be tried."
        },
        {
          "name": "max_epsilon",
          "type": "int",
          "default": 1,
          "required": true,
          "doctxt": "Largest step size."
        }
      ]
    },
    {
      "name": "IterativeGradientSignAttack",
      "extensions": [
        {
          "extension": "jpg"
        }
      ],
      "doctxt": "Like GradientSignAttack but with several steps for each epsilon.",
      "options": [
        {
          "name": "model",
          "type": "enum",
          "values": [
            "vgg16",
            "resnet18"
          ],
          "required": true,
          "doctxt": "Pre-trained model to be attacked."
        },
        {
          "name": "epsilons",
          "type": "int",
          "default": 100,
          "required": true,
          "doctxt": "Either Iterable of step sizes in the direction of the sign of the gradient or number of step sizes between 0 and max_epsilon that should be tried."
        },
        {
          "name": "max_epsilon",
          "type": "int",
          "default": 1,
          "required": true,
          "doctxt": "Largest step size if epsilons is not an iterable."
        },
        {
          "name": "steps",
          "type": "int",
          "default": 10,
          "required": true,
          "doctxt": "Number of iterations to run."
        }
      ]
    },
    {
      "name": "ProjectedGradientDescent",
      "extensions": [
        {
          "extension": "jpg"
        }
      ],
      "doctxt": "The Projected Gradient Attack introduced in [1] with random start.\n\nWhen used without a random start, this attack is also known as Basic Iterative Method (BIM) or FGSM^k.\n\nReferences\n[1]\tAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu,\n\"Towards Deep Learning Models Resistant to Adversarial Attacks\", https://arxiv.org/abs/1706.06083",
      "options": [
        {
          "name": "model",
          "type": "enum",
          "values": [
            "vgg16",
            "resnet18"
          ],
          "required": true,
          "doctxt": "Pre-trained model to be attacked."
        },
        {
          "name": "binary_search",
          "type": "bool",
          "default": true,
          "required": true,
          "doctxt": "Whether to perform a binary search over epsilon and stepsize, keeping their ratio constant and using their values to start the search. If False, hyperparameters are not optimized. Can also be an integer, specifying the number of binary search steps (default 20)."
        },
        {
          "name": "epsilon",
          "type": "float",
          "default": 0.3,
          "required": true,
          "doctxt": "Limit on the perturbation size; if binary_search is True, this value is only for initialization and automatically adapted."
        },
        {
          "name": "stepsize",
          "type": "float",
          "default": 0.01,
          "required": true,
          "doctxt": "Step size for gradient descent; if binary_search is True, this value is only for initialization and automatically adapted."
        },
        {
          "name": "iterations",
          "type": "int",
          "default": 40,
          "required": true,
          "doctxt": "Number of iterations for each gradient descent run."
        },
        {
          "name": "random_start",
          "type": "bool",
          "default": false,
          "required": true,
          "doctxt": "Start the attack from a random point rather than from the original input."
        },
        {
          "name": "return_early",
          "type": "bool",
          "default": true,
          "required": true,
          "doctxt": "Whether an individual gradient descent run should stop as soon as an adversarial is found."
        }
      ]
    },
    {
      "name": "SaltAndPepperNoiseAttack",
      "extensions": [
        {
          "extension": "jpg"
        }
      ],
      "doctxt": "Increases the amount of salt and pepper noise until the input is misclassified.",
      "options": [
        {
          "name": "model",
          "type": "enum",
          "values": [
            "vgg16",
            "resnet18"
          ],
          "required": true,
          "doctxt": "Pre-trained model to be attacked."
        },
        {
          "name": "epsilons",
          "type": "int",
          "default": 100,
          "required": true,
          "doctxt": "Number of steps to try between probability 0 and 1."
        },
        {
          "name": "repetitions",
          "type": "int",
          "default": 10,
          "required": true,
          "doctxt": "Specifies how often the attack will be repeated."
        }
      ]
    },
    {
      "name": "SinglePixelAttack",
      "extensions": [
        {
          "extension": "jpg"
        }
      ],
      "doctxt": "Perturbs just a single pixel and sets it to the min or max.",
      "options": [
        {
          "name": "model",
          "type": "enum",
          "values": [
            "vgg16",
            "resnet18"
          ],
          "required": true,
          "doctxt": "Pre-trained model to be attacked."
        },
        {
          "name": "max_pixels",
          "type": "int",
          "default": 1000,
          "required": true,
          "doctxt": "Maximum number of pixels to try."
        }
      ]
    }
  ],
  "doctxt": "Foolbox is a Python toolbox to create adversarial examples that fool neural networks.",
  "tagline": "Fool neural networks!"
}